# ðŸ’¬ ChatGPDiss - ChatGPT Tweet Sentiment Analysis ðŸ§ 

This project trains and evaluates deep learning models for **multiclass tweet sentiment analysis** (Bad, Neutral and Good), using **PyTorch**, **Streamlit** for the interative interface, and visualizations with `matplotlib` and `great_tables`.

---

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ graphs/                    # Saved plots
â”œâ”€â”€ models/                    # Trained models (.pt)
â”œâ”€â”€ neural_networks/           # Model architecture definitions (RNN, LSTM)
â”œâ”€â”€ nlp/                       # NLP preprocessing utilities
â”œâ”€â”€ presentation/              # Streamlit app interface
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ results/                   # Output from experiments
â”œâ”€â”€ notebook.ipynb             # Main notebook with training, evaluation, and visualization
â”œâ”€â”€ pyproject.toml             # Project metadata and dependencies
â””â”€â”€ uv.lock                    # uv dependency lock file
```

---

## ðŸ› ï¸ Requirements

Install dependencies using [uv](https://github.com/astral-sh/uv), a fast Python package manager:

```bash
uv venv                     # Create and activate virtual environment
source .venv/bin/activate   # or .venv\Scripts\activate on Windows
uv sync                     # Install dependencies
```

> To save tables with `great_tables`, the `weasyprint` package is also required (and may need system dependencies like Cairo and Pango).

All of the data is imported using `kagglehub`.

---

## ðŸš€ How to Run

1. Make sure you have installed all dependencies using `uv` as described above.
2. Run the Streamlit app:

```bash
streamlit run presentation/app.py
```

3. A browser window should open at `http://localhost:8501` showing the interactive interface.

---

## ðŸ”„ How to Reproduce Results

- **Training**: run `notebook.ipynb` to preprocess the data, train both RNN and LSTM classifiers for sentiment analysis, and save the best model checkpoints to the `models/` folder.
- **Evaluation**: accuracy is computed on the training and test sets at each epoch, and key metrics are visualized using `matplotlib`.
- **Classification Report**: rendered as styled tables using `great_tables`, summarizing precision, recall, and F1-score for each sentiment class.

---

## ðŸ“Š Results

- **Final accuracy on holdout**: 91.25%
- **Best model**: `LSTM`
- **F1-Scores**:
  - Bad: 0.93
  - Neutral: 0.88
  - Good: 0.92
